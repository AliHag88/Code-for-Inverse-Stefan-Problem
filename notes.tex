\documentclass[letterpaper, 10pt, draft]{amsart}

\usepackage[T1]{fontenc}
\usepackage{graphicx, baskervald}

\usepackage{todonotes}

\usepackage{geometry}

\usepackage{matlab-prettifier}

\usepackage[inline,draft]{showlabels}

\let\institute\address%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newcommand{\D}[2]{\frac{\partial{} #1}{\partial{} #2}}
\newcommand{\dD}[2]{\frac{d #1}{d #2}}
\newcommand{\bk}[1]{\left\{#1\right\}}
\newcommand{\norm}[1]{\left\Vert #1\right\Vert}%chktex 1 ok (math-mode)
\newcommand{\lnorm}[1]{\left\vert #1\right\vert}%chktex 1 ok (math-mode)
\DeclareMathOperator{\arccoth}{\text{arccoth}}
\let\l\ell%

\begin{document}

\title{ISP Implementation Notes}
\author{Ali Haqverdiyev \and Jonathan Goldfarb}
\institute{Department of Mathematical Sciences\\Florida Institute of
    Technology\\Melbourne, FL 32901}
\subjclass[2010]{TBD}
\date{\today}

% \begin{abstract}
%     TBD

%     \smallskip%
%     \noindent\textbf{\keywordsname.} TBD
% \end{abstract}
\maketitle

\tableofcontents{}

\section{Introduction}\label{sec:introduction}

\subsection{Inverse Stefan Problem}\label{sec:inverse-stefan-problem}
Consider the general one-phase Stefan problem:
\begin{gather}
  Lu \equiv {(a(t) b(x) u_x)}_x - u_{t} = f,~\text{in}~\Omega \label{eq:intro-pde}
  \\
  u(x,0) = \phi (x),~0 \leq x \leq s(0)=s_0 \label{eq:intro-init}
  \\
  a(t) b(0) u_x (0,t) = g(t),~0 \leq t \leq T \label{eq:pde-bound}
  \\
  a(t) b(s(t)) u_x (s(t),t)
  + \gamma (s(t),t) s'(t)
  - \chi (s(t),t) = 0,~0 \leq t \leq T \label{eq:pde-stefan}
  \\
  u(s(t),t) = \mu (t),~0 \leq t \leq T, \label{eq:pde-freebound}
  \intertext{where}
  \Omega = \{(x,t): 0 < x < s(t),~0 < t \leq T\}\label{eq:pde-domain-defn}
\end{gather}
where $a$, $b$, $f$, $\phi$, $g$, $\gamma$, $\chi$, $\mu$ are given functions.
Assume now that $a$ is unknown; in order to find $a$, along with $u$ and $s$, we must have additional information.
Assume that we are able to measure the temperature on our domain and the position of the free boundary at the final moment $T$.
\begin{equation}
    u(x,T) = w(x),~ 0 \leq x \leq s(T) = s_*.\label{eq:pde-finaltemp}
\end{equation}
Under these conditions, we are required to solve an \emph{inverse} Stefan problem (ISP): find a tuple
\def\controlvarsWithArgs{s(t), a(t)}
\def\controlvars{s, a}
\def\controlvarsWithDelta{{\Delta s}, {\Delta a}}%chktex 1 ok (math-mode)
\def\acontrolspace{W_2^1}
\def\scontrolspace{W_2^2}
\def\controlSpace{\scontrolspace(0,T) \times{} \acontrolspace(0,T)}
\def\controlSpaceNorm{ \max\Big( %
    \norm{s}_{\scontrolspace(0,T)}, %
    \norm{a}_{\acontrolspace(0,T)} %
    \Big)}
\[
    \bk{
        u(x,t), \controlvarsWithArgs
    }
\]
that satisfy conditions~\eqref{eq:intro-pde}--\eqref{eq:pde-finaltemp}.
ISP is not well posed in the sense of Hadamard: the solution may not exist; if
it exists, it may not be unique, and in general it does not exhibit continuous
dependence on the data.
The main methods available for ISP are based on a variational formulation,
Frechet differentiability, and iterative gradient methods.
We cite recent papers~\cite{abdulla13,abdulla15} and the
monograph~\cite{goldman97} for a list of references.
The established variational methods in earlier works fail in general to address
two issues:
\begin{itemize}
    \item The solution of ISP does not depend continuously on the phase
      transition temperature $\mu(t)$ from~\eqref{eq:pde-freebound}.
    A small perturbation of the phase transition temperature may imply
    significant change of the solution to the inverse Stefan problem.
    \item In the existing formulation, at each step of the iterative method a
      Stefan problem must be solved (that is, for instance, the unknown heat
      flux $g$ is given, and the corresponding $u$ and $s$ are calculated) which
      incurs a high computational cost.
\end{itemize}

A new method developed in~\cite{abdulla13,abdulla15} addresses both issues with
a new variational formulation.
The key insight is that the free boundary problem has a similar nature to an
inverse problem, so putting them into the same framework gives a conceptually
clear formulation of the problem; proving existence, approximation, and
differentiability is a resulting challenge.
Existence of the optimal control and the convergence of the sequence of discrete
optimal control problems to the continuous optimal control problem was proved
in~\cite{abdulla13,abdulla15}.
In~\cite{abdulla16,abdulla17}, Frechet differentiability of the new variational
formulation was developed, and a full result showing Frechet differentiability
and the form of the Frechet differential
with respect to the free boundary, sources, and coefficients were proven.
Lastly, the gradient method was implemented in~\cite{abdulla18} and numerical
results were proven.
Our goal in this work is to extend the implementation to the identification of
the diffusion coefficient $a(t)$ and document the implementation in MATLAB using the built-in PDEPE solver.

The rest of this note is as follows:

\section{Variational Formulation}\label{sec:variational-formulation}
We will consider the problem~\eqref{eq:intro-pde}--\eqref{eq:pde-stefan} where $(\controlvars{}) \in V_R$ is fixed, and
\begin{align}
  V_R & =\Big\{
        v=(\controlvars{}) \in H, \lnorm{v}_H \leq R;
        ~s(0) = s_0,
        ~g(0) = a(0)b(0) \phi'(0),
        ~a_0 \leq a(t)\nonumber
  \\
      & \qquad
        ~\chi(s_{0},0) = \phi'(s_{0})a(0)b(s_0) + \gamma(s_0, 0)s'(0),
        ~0 < \delta \leq s(t)
        \Big\},\label{eq:control-set}
\end{align}
where $\beta_0, \beta_1, \beta_2 \geq 0$ and $a_0, \delta, R > 0$ are given, and
\begin{gather*}
  H := \controlSpace{}
    \\
    \norm{v}_H := \controlSpaceNorm{}
  \end{gather*}
  Define
\[
  D := \bk{(x,t) : 0\leq x\leq \l,~ 0\leq t\leq T},
\]
where $l = l(R) > 0$ is chosen such that for any control $v\in V_R$, its component
$s$ satisfies $s(t)\leq l$.

Consider the minimization of the functional
\def\J{\mathcal{J}}
\begin{equation}
  \J(v)
  = \beta_0 \int_0^{s(T)} \lnorm{u(x, T; v) - w(x)}^2\,dx
  + \beta_1 \int_0^T \lnorm{u(s(t), t; v) - \mu(t)}^2\,dt
  + \beta_2 \lnorm{s(T) - s_*}^2\label{eq:functional}
\end{equation}
on the control set~\eqref{eq:control-set}.
\def\I{\mathcal{I}}
The formulated optimal control problem~\eqref{eq:control-set},~\eqref{eq:tform-pde}--\eqref{eq:tform-rbdy},~\eqref{eq:functional} will be called Problem $\I$.

\subsection{Gradient Result}\label{sec:frechet-gradient}

\begin{definition}\label{defn:adjoint}
  For given $v$ and $u = u(x, t; v)$, $\psi$ is a solution to the adjoint problem if
  \begin{gather}
    L^{*} \psi := \big(a b \psi_x\big)_x + \psi_t = 0,\quad\text{in}~\Omega \label{eq:adj-pde}
    \\
    \psi(x, T) = 2\beta_0(u(x, T) - w(x)),~0 \leq x \leq s(T) \label{eq:adj-finalmoment}
    \\
    b(0)a(t)\psi_x(0, t) =0,~0 \leq t \leq T \label{eq:adj-robin-fixed}
    \\
    \Big[a b \psi_x - s'\psi - 2\beta_1(u - \mu)\Big]_{x=s(t)}=0, ~0 \leq t \leq T \label{eq:adj-robin-free}
  \end{gather}
\end{definition}


\begin{theorem}\label{thm:gradient-result}
  Problem $\I$ has a solution, and the functional $\J(v)$ is differentiable in the sense of Frechet, and the first variation is
  \begin{gather}
    \left\langle \J'(v),\Delta v \right\rangle_H
    = \int_0^T \Big[2\beta_1\big(u - \mu\big)u_x + \psi \big(\chi_x - \gamma_x s'  - a (b u_x)_x \big)\Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
    \\
    +
    \left[\beta_0\lnorm{u(s(T),T) - w(s(T))}^2 + 2 \beta_2 (s(T) - s_*)\right] {\Delta s}(T) 
    - \int_0^T \big[\psi \gamma\big]_{x=s(t)} {\Delta s}'(t) \,dt \nonumber
    \\
    +\int_0^T {\Delta a} \left[ \int_0^{s(t)} {(b u_x)}_x \psi \,dx - [b u_x\psi]_{x=s(t)} - [b u_x \psi]_{x=0}\right] \,dt\label{eq:gradient-full}
  \end{gather}
  where $\J'(v) \in H'$ is the Frechet derivative, $\langle \cdot,\cdot \rangle_H$ is a pairing between $H$ and its dual $H'$, $\psi$ is a solution to the adjoint problem in the sense of definition~\ref{defn:adjoint}, and $\delta v = (\controlvarsWithDelta{})$ is a variation of the control vector $v \in V_R$ such that $v + \delta v \in V_R$.
\end{theorem}

\subsection{General Form of PDE Problem}
Both the forward problem~\eqref{eq:intro-pde}--\eqref{eq:pde-stefan} and the
adjoint problem~\eqref{eq:adj-pde}--\eqref{eq:adj-robin-free} have a common
structure that can be exploited for implementation in code.
Consider the problem
\begin{gather}
  (a(t) b(x) u_x)_x  - u_t = f(x,t),~0<x<s(t),~0<t<T\label{eq:general-pde}
  \\
  u(x,0) = \phi(x),~0<x<s(0)\label{eq:general-init}
  \\
  a(t)b(0) u_x(0,t) = g(t),~0<t<T\label{eq:general-fixedbdy}
  \\
  a(t)b(s(t)) u_x(s(t),t) + r(t) u(s(t),t) - p(t)=0,~0<t<T\label{eq:general-movingbdy}
\end{gather}
For the forward problem, we have $r(t) \equiv 0$ and $p(t) = \gamma(s(t),t)-\chi(s(t),t)s'(t)$.
Under the change of variables $\bar{t} = T-t$, the
problem~\eqref{eq:adj-pde}--\eqref{eq:adj-robin-free} is of the same form after
taking $s(t) = s(T-t)$, $a(t)=a(T-t)$, etc.\@
The initial condition becomes $\phi(x)=2 \beta_0 (u(x,T) - w(x))$, the heat flux
on the fixed boundary is $g(t)\equiv 0$, the coefficient $r(t) = -s'(t)$, and
the function $p(t) = -2\beta_1 (u - \mu)$.

In this work, we consider a model with reduced complexity in order to test the
main algorithm in isolation; in particular, we take $\beta_i \equiv 1$, $b\equiv
1$, $\gamma \equiv 1$, $\chi \equiv 0$.

To solve the problem~\eqref{eq:general-pde}--\eqref{eq:general-movingbdy} for a
fixed boundary curve $x=s(t)$ corresponding to either of the two situaions
described above, we transform the domain $\Omega$ to the cylindrical domain
$Q_T = (0,1)\times (0,T)$ by the change of variables $y=x/s(t)$.
Let $d = d(x, t)$, $(x, t) \in \Omega$ stand for any of $u$, $f$, $\gamma$, $\chi$, define the function $\tilde{d}$ by
\begin{gather*}
  \tilde{d}(y,t) = d\big(y s(t), t\big),~
  \tilde{b}(y) = b(y s(t)),~\text{and}~
  \tilde{\phi}(y) = \phi\big(y s_0\big)
\end{gather*}
\def\utilde{\tilde{u}}
The transformed function $\utilde$ is a \emph{pointwise a.e.} solution of the
Neumann problem
\begin{gather}
  \frac{a(t)}{s^2(t)} {(b(ys(t)) u_y)}_y
  - \left(u_t - \frac{y s'(t)}{s(t)} u_y\right)= f(ys(t),t),~\text{in}~Q_T\nonumber
  \\
  u(y,0) = \phi(ys(t)),~0<y<1\nonumber
  \\
  \frac{a(t)b(0)}{s(t)} u_y(0,t) = g(t),~0<t<T\nonumber
  \\
  \frac{a(t)b(s(t))}{s(t)} u_y(1,t) + r(t) u(1,t) - p(t)=0,~0<t<T\nonumber
\end{gather}
which can be written in the form
\begin{gather}
  s u_t = \frac{a}{s} {(\tilde{b} u_y)}_y
  + y s' u_y
  - s \tilde{f}(y,t),~\text{in}~Q_T\label{eq:tform-pde}
  \\
  \utilde = \tilde{\phi}, ~0 \leq y \leq 1 \label{eq:tform-iv}
  \\
  \frac{a \tilde{b}}{s} \utilde_y - g \vert_{y=0}= 0, ~0 \leq t \leq T   \label{eq:tform-lbdy}
  \\
  \frac{a\tilde{b}}{s} u_y + r u - p \vert_{y=1} = 0, ~0 \leq t \leq T \label{eq:tform-rbdy}
\end{gather}


\section{Details of MATLAB Implementation}
The built-in MATLAB PDE solver, \verb+pdepe+, solves parabolic-elliptic problems in one space dimension.
In particular, it uses a method-of-lines technique and a differential-algebraic equation solver applied to problems of the form
\begin{gather}
  c\left(y,t,u,\D{u}{y}\right) \D{u}{t}
  = y^{-m} \D{}{y} \left( y^m F\left(y,t,u,\D{u}{x} \right)\right)
  + d\left( y,t,u,\D{u}{y}\right)\label{eq:matlab-pde}
  \intertext{for}
  a \leq y \leq b,\quad t_0 \leq t \leq t_f
  \intertext{where $m=0,1,2$ is fixed, $c$ is a diagonal matrix of size $n\times n$, where there are $n$ components in $u$.
    There must be at least one parabolic equation, which corresponds to the condition of at least one component of $c$ being positive.
    The solution components satisfy}
  u(y,t_0) = u_0(y),~a \leq y \leq b\label{eq:matlab-ic}
  \intertext{and boundary conditions of the form}
  p(y,t,u)
  + q(y,t) F\left(y,t,u,\D{u}{y} \right)\Big\vert_{y=a,b}
  = 0\label{eq:matlab-bc}
\end{gather}
In particular, the function $F$ appearing in the boundary condition is the same
as the flux term in the PDE.\@
Choose $m=0$, $a=0$, $b=1$, $t_0=0$, $t_f=T$ and compare~\eqref{eq:tform-pde}--\eqref{eq:tform-rbdy} to~\eqref{eq:matlab-pde}--\eqref{eq:matlab-bc} to find
\begin{gather*}
  c(y,t,u,u_y) := s(t)
  \\
  F(y,t,u,u_y) := \frac{\tilde{b}(y) a(t)}{s(t)} u_y
  \\
  c(y,t,u,u_y) := y s'(t) u_y - s \tilde{f}(y,t)
  \\
  q(y,t) := 1
  \\
  p(y,t,u) := \begin{cases}
    -g,~&y=0
    \\
    r u - p,~&y=1
  \end{cases}
\end{gather*}

The function \verb+pdeSolver+, included in
Section~\ref{sec:code-listing-pdeSolver}, solves problems in the previous
formulation with $f\equiv 0$ and $\tilde{b} \equiv 1$.
The necessary setup and calculation of traces for the forward problem are
completed in the function \verb+Forward+, included in Section~\ref{sec:code-listing-forward}. 
The model problem currently implemented in \verb+test_Forward+, included in Section~\ref{sec:code-listing-test-forward} can be found in Section~\ref{sec:model-problem-1}.

The functional $J$ and the calculation of the state vector is completed in the
function \verb+Functional+, included in Section~\ref{sec:code-listing-functional}.

The corresponding routines for the adjoint problem are implemented in \verb+Adjoint+, included in Section~\ref{sec:code-listing-adjoint}.
The same problem is used in \verb+test_Forward+ and in \verb+test_Adjoint+
(taking the appropriate boundary measurements; see
Section~\ref{sec:code-listing-test-adjoint}) and we simply check that the
adjoint problem vanishes as synthetic error introduced in the measurements from
the forward problem vanishes.

\subsection{Implementation of Gradient Formulae in MATLAB}
The last step in the implementation is to write the gradient update formula in MATLAB notation.
For the differential with respect to $x=s(t)$, we have
\begin{gather}
  {\delta J}_{s}
  := \int_0^T \Big[2\beta_1\big(u - \mu\big)u_x + \psi \big(\chi_x - \gamma_x s'  - a (b u_x)_x \big)\Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  \\
  +
  \left[\beta_0\lnorm{u(s(T),T) - w(s(T))}^2 + 2 \beta_2 (s(T) - s_*)\right] {\Delta s}(T) 
  - \int_0^T \big[\psi \gamma\big]_{x=s(t)} {\Delta s}'(t) \,dt\label{eq:gradient-wrt-s-original}
\end{gather}
Under the assumptions used to formulate ``reduced'' models above ($\gamma \equiv
1$, $\chi \equiv 0$, $b\equiv 1$, $\beta_i \equiv 1$) we have
\begin{gather}
  {\delta J}_{s}
  = \int_0^T \Big[2\big(u - \mu\big)u_x - \psi  a u_{xx} \Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  \\
  +
  \left[\lnorm{u(s(T),T) - w(s(T))}^2 + 2 (s(T) - s_*)\right] {\Delta s}(T) 
  - \int_0^T \psi(s(t),t) {\Delta s}'(t) \,dt\label{eq:gradient-wrt-s-reduced}
\end{gather}
Pursuing integration by parts with respect to time, it follows that the corresponding differential term is
\begin{gather}
  % \int_0^T \Big[2\big(u - \mu\big)u_x - \psi  a u_{xx} \Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  % \\
  % +
  % \left[\lnorm{u(s(T),T) - w(s(T))}^2 + 2 (s(T) - s_*)\right] {\Delta s}(T) 
  % + \int_0^T \D{}{t}\psi(s(t),t) {\Delta s}(t) \,dt
  % \\
  % - \psi(s(T),T) {\Delta s}(T)
  % + \psi(s(0),0) {\Delta s}(0)
  % \intertext{and hence}
  % {\delta J}_{s}
  % =\int_0^T \Big[2\big(u - \mu\big)u_x - \psi  a u_{xx} \Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  % \\
  % +
  % \left[\lnorm{u(s(T),T) - w(s(T))}^2 + 2 (s(T) - s_*) - \psi(s(T),T)\right] {\Delta s}(T) 
  % + \int_0^T \D{}{t}\psi(s(t),t) {\Delta s}(t) \,dt
  % \\
  % {\delta J}_s
  % = \int_0^T \Big[2\big(u - \mu\big)u_x - \psi  a u_{xx}
  % + \psi_x(s(t),t) s'(t) + \psi_t(s(t),t)\Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  % \\
  % +
  % \left[
  %   \lnorm{u(s(T),T) - w(s(T))}^2 + 2 (s(T) - s_*) - \psi(s(T),T)
  % \right] {\Delta s}(T)
  % \intertext{Since $\psi(x,T) \equiv 2(u(x,T) - w(x))$, this expression can be
  %   written as}
  % {\delta J}_s
  % = \int_0^T \Big[2\big(u - \mu\big)u_x - \psi  a u_{xx}
  % + \psi_x(s(t),t) s'(t) + \psi_t(s(t),t)\Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  % \\
  % +
  % \left[
  %   \lnorm{u(s(T),T) - w(s(T))}^2 + 2 (s(T) - s_*) - 2(u(s(T),T) - w(s(T))
  % \right] {\Delta s}(T)
  % \intertext{and hence}
  {\delta J}_s
  =\int_0^T \Big[2\big(u - \mu\big)u_x - \psi  a u_{xx}
  + \psi_x s'(t) + \psi_t\Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  \\
  +
  \left[
    \big[u - w\big]\big[u - w - 2\big] + 2 (s(T) - s_*)
  \right]\vert_{(x,t)=(s(T),T)} {\Delta s}(T)\label{eq:gradient-wrt-s}
\end{gather}
Interpreting the integrand in the first term as a pointwise function is one way
to write the gradient calculation in discrete form; this is the routine implemented in \verb+grad_s+, included in Section~\ref{sec:code-listing-grad-s}.

The differential with respect to $a(t)$ is given by
\begin{gather*}
  \delta J_a := \int_0^T {\Delta a} \left[
    \int_0^{s(t)}  u_{xx} \psi \,dx
    - [u_x \psi]_{x=0}
    - [u_x \psi]_{x=s(t)}
  \right]\,dt
\end{gather*}
Note that
\begin{gather}
  \int_0^{s(t)} u_{xx} \psi \,dx
  = - \int_0^{s(t)} u_x \psi \,dx
  + u_x(s(t),t) \psi(s(t),t)
  - u_x(0,t) \psi(0,t)
  \intertext{so the gradient with respect to $a$ can be written in the form}
    J_a(t) = - \int_0^{s(t)}  u_{x} \psi_x \,dx
    - 2 [u_x \psi]_{x=0}\label{eq:gradient-wrt-a}
\end{gather}

This final form of the gradient with respect to $a$ is implemented in
\verb+grad_a.m+, included below in Section~\ref{sec:code-listing-grad-a}.

\subsection{Sobolev Preconditioning}
Note that the gradient
formulae~\eqref{eq:gradient-wrt-s},~\eqref{eq:gradient-wrt-a} do not, in
general, agree precisely with the definition of the Frechet gradient of $s$ and
$a$, respectively; for example, since $s \in \scontrolspace$, we should require
the gradient $J_s$ be in the form
\begin{align*}
  \J(s+{\Delta s}) - \J(s)
  & = \langle J_s, {\Delta s} \rangle_{\scontrolspace(0,T)}
  + o({\Delta s})
  \\
  & = \int_0^T \left[
    J_s(t) {\Delta s}(t)
    + J_s'(t) {\Delta s}'(t)
    + J_s''(t) {\Delta s}''(t)
    \right]\,dt
    + o({\Delta s})
\end{align*}
but the differential of eq.~\eqref{eq:gradient-wrt-s} has the form
\[
  {\delta J}_s = \int_0^T \tilde{J}_s(t) {\Delta s}(t) \,dt + \tilde{\tilde{J}} {\Delta s}(T)
\]
Note also that the above functional \emph{is} formally a linear functional over
$\scontrolspace{}$; functions in $\scontrolspace(0,T)$ are continuous, so the
delta function is a member of the dual space.
By Reisz representation formula, any such functional over $\scontrolspace$ can
be written in the required form; what remains is to numerically approximate the
Reisz element $J_s$ of the gradient, which is referred to as \emph{preconditioning}
(in particular, Sobolev preconditioning) the gradient.
This technique is discussed in detail in the monograph~\cite{neuberger10}.

This process is shown to be essential for the numerical results; without
preconditioning, the original set of ``$L_2$'' gradients do not give a clear
enough search direction for the gradient descent process.
The this process can be thought of as a mathematically motivated preconditioning
or smoothing operation.

Since the gradient is used to derive the search direction in the gradient
descent algorithm, one should require that the action of $J_s$ on the increment
${\Delta s}$ be unchanged after this projection.
We will for now project $s$ and $a$ into $W_2^1$; we should have
\begin{gather*}
  \int_0^T \left[
    \tilde{J}_s(t) {\Delta s}(t)
    + \delta_T(t)\tilde{\tilde{J}} {\Delta s}(T)
  \right]\,dt
  =\int_0^T \left[
    J_s(t) {\Delta s}(t)
    + J_s'(t) {\Delta s}'(t)
  \right] \,dt
  \\
  = \int_0^T \left[
    J_s(t) {\Delta s}(t)
    - J_s''(t) {\Delta s}(t)
    + J_s'(T) {\Delta s}(T)\delta_T(t)
    - J_s'(0) {\Delta s}(0)\delta_0(t)
  \right] \,dt
  \intertext{and hence}
  % 0 = \int_0^T \left[
  %   J_s(t) {\Delta s}(t)
  %   - J_s''(t) {\Delta s}(t)
  %   + J_s'(T) {\Delta s}(T)\delta_T(t)
  %   - J_s'(0) {\Delta s}(0)\delta_0(t)
  %   - \tilde{J}_s(t) {\Delta s}(t)
  %   - \delta_T(t)\tilde{\tilde{J}} {\Delta s}(T)
  % \right] \,dt
  % \\
  0 = \int_0^T \left[
    \big(J_s(t) - \tilde{J}_s(t)\big) 
    - J_s''(t) \right]
  {\Delta s}(t) \,dt
  + \big(J_s'(T) - \tilde{\tilde{J}}\big) {\Delta s}(T)
  - J_s'(0) {\Delta s}(0)
\end{gather*}
By the arbitraricity of $\Delta s$, it follows that 
\begin{gather*}
  J_s(t) - J_s''(t) = \tilde{J}_s(t)
  \\
  J_s'(T) = \tilde{\tilde{J}},\quad
  % \\
  J_s'(0) = 0
\end{gather*}
In practice, we may choose to penalize large gradient values by including a
parameter $\ell>0$ in the definition of the $W_2^1$ norm, which requires
the solution of the alternative equation
\[
  \int_0^T \left[
    \tilde{J}_s(t) {\Delta s}(t)
    + \delta_T(t)\tilde{\tilde{J}} {\Delta s}(T)
  \right]\,dt
  =\int_0^T \left[
    J_s(t) {\Delta s}(t)
    + \ell J_s'(t) {\Delta s}'(t)
  \right] \,dt
\]
which implies $J_s$ should satisfy
\begin{equation*}
  \begin{cases}
    J_s(t) - \ell J_s''(t) = \tilde{J}_s(t)
    \\
    \ell J_s'(T) = \tilde{\tilde{J}},\quad
    % \\
    J_s'(0) = 0
  \end{cases}
\end{equation*}
Note that the formulation used to approximate the Reisz element in~\cite{abdulla18} would read
\begin{equation*}
  \begin{cases}
    J_s(t) - \ell J_s''(t) = \tilde{J}_s(t) + \tilde{\tilde{J}}
    \\
    J_s'(T) = 0,\quad
    % \\
    J_s'(0) = 0
  \end{cases}
\end{equation*}
in our notation, which will give significantly different results whenever
$\tilde{\tilde{J}}\neq 0$.
Even this formulation does not take into account one condition
from~\eqref{eq:control-set}; for a fixed control $a(t)$, given any two controls
$s$, $\tilde{s}$ should satisfy $s(0)=s_0=\tilde{s}(0)$, so ${\Delta s}(0) = 0$.
Further, both satisfy the compatibility condition and hence
\begin{gather*}
  % \chi(s_{0},0) = \phi'(s_{0})a(0)b(s_0) + \gamma(s_0, 0)s'(0)
  % \\
  % \chi(s_{0},0) = \phi'(s_{0})a(0)b(s_0) + \gamma(s_0, 0)\tilde{s}'(0),
  % \intertext{which implies}
  {\Delta s}'(0) = \tilde{s}'(0) - s'(0) = 0
\end{gather*}
Since ${\Delta s}(0)$ is not actually arbitrary, we are \emph{not}, in general,
required to that $J_s'(0)=0$.
Given that $s(0)=s_0$ is fixed, it is justified to instead solve
\begin{equation}
  \begin{cases}
  J_s(t) - \ell J_s''(t) = \tilde{J}_s(t)
  \\
  \ell J_s'(T) = \tilde{\tilde{J}},\quad
  % \\
  J_s(0) = 0
\end{cases}\label{eq:gradient-s-precond}
\end{equation}

Making a similar calculation for $a(t)$, we see that the relationship between
the preconditioned $(H_1)$ gradient $J_a$ and the original $(L_2)$ gradient
$\tilde{J}_a$ should be
\begin{equation}
  \begin{cases}
  J_a(t) - \ell J_a''(t) = \tilde{J}_a(t)\nonumber
  \\
  \ell J_a'(T) = 0,\quad
  % \\
  J_a(0) = 0
  \end{cases}\label{eq:gradient-a-precond}
\end{equation}
As in the previous case, the Dirichlet condition at $x=0$ is justified by the
lack of arbitrarity in $a(0)$ due to~\eqref{eq:control-set}.

The transformation applied to the gradient with respect to $s(t)$ between
eqs.~\eqref{eq:gradient-wrt-s-reduced} and~\eqref{eq:gradient-wrt-s} can be
reformulated into this preconditioning step as well; note
that~\eqref{eq:gradient-wrt-s-reduced} is of the form
\begin{gather*}
  {\delta J}_s
  = \int_0^T \left[
    J_1 {\Delta s} +
    J_2 {\Delta s}' +
    J_3 {\Delta s} \delta_T(t)
  \right]
  \,dt
  \intertext{and hence we require}
  \int_0^T \left[
    J_1 {\Delta s} +
    - J_2' {\Delta s} +
    J_3 {\Delta s} \delta_T(t)
  \right]
  \,dt
  =
  \int_0^T \left[
    J_s(t) {\Delta s}(t)
    + \ell J_s'(t) {\Delta s}'(t)
  \right] \,dt
  \\
  % =\int_0^T \left[
  %   J_s {\Delta s}
  %   - \ell J_s'' {\Delta s}
  %   + \ell J_s' {\Delta s} \delta_T
  %   - \ell J_s' {\Delta s} \delta_0
  % \right] \,dt
  % \\
  =\int_0^T \left[
    J_s {\Delta s}
    - \ell J_s'' {\Delta s}
    + \ell J_s' {\Delta s} \delta_T
  \right] \,dt
  \intertext{and hence}
  0 = \int_0^T \left[
    J_1 - J_2' - J_s + \ell J_s''
  \right] {\Delta s}
  + \left[
    J_3 - \ell J_s'
  \right] {\Delta s} \delta_T(t) \,dt
\end{gather*}
and hence we could solve the single problem
\begin{equation}
  \begin{cases}
    \ell J_s'' - J_s = J_1 - J_2'
    \\
    \ell J_s'(T) = J_3
    \\
    J_s(0) = 0
  \end{cases}
\end{equation}

Note that in order to solve these problems numerically, they are reformulated as
a first order system; for example letting $y_1 = J_s(t)$, $y_2 = y_1'(t)$,
eq.~\eqref{eq:gradient-s-precond} becomes
\begin{equation}
  \begin{cases}
    \begin{bmatrix}
      y_1
      \\
      y_2
    \end{bmatrix}'
    = \begin{bmatrix}
      y_2
      \\
      \frac{y_1 - \tilde{J}_s(t)}{\ell}
    \end{bmatrix}
    \\
    \ell y_2(T) = \tilde{\tilde{J}}
    \\
    y_1(0) = 0
  \end{cases}
\end{equation}

This is the formulation of the preconditioning problem that is currently output
by our preconditioning code \verb+precond.m+, included below in
Section~\ref{sec:code-listing-precond}; the original problem is selected by
choosing \verb+mode+ to be 1, and the updated problem is selected by choosing
\verb+mode+ to be 2.

\section{Model Problem \#1}\label{sec:model-problem-1}

\subsection{Problem Setup and Generation of Test Data}

As a model problem, we will take~\eqref{eq:intro-pde}--\eqref{eq:pde-stefan}
with $\beta_i \equiv 1$, $b\equiv 1$, $\chi \equiv 0$, $\gamma \equiv 1$, and
$f\equiv 0$:
\begin{gather}
  Lu \equiv {(a(t) u_x)}_x - u_{t} = 0,~\text{in}~\Omega
  \\
  u(x,0) = \phi (x),~0 \leq x \leq s(0) = s_0
  \\
  a(t) u_x (0,t) = g(t),~0 \leq t \leq T
  \\
  a(t) u_x (s(t),t) + s'(t) = 0,~0 \leq t \leq
  T
\end{gather}
In this case, the adjoint problem~\eqref{eq:adj-pde}--\eqref{eq:adj-robin-free} then takes the form
\begin{gather}
  L^{*} \psi := \big(a \psi_x\big)_x + \psi_t = 0,\quad\text{in}~\Omega
  \\
  \psi(x, T) = 2(u(x, T) - w(x)),~0 \leq x \leq s(T)
  \\
  a(t)\psi_x(0, t) =0,~0 \leq t \leq T
  \\
  \Big[a \psi_x - s'\psi - 2 (u - \mu)\Big]_{x=s(t)}=0, ~0 \leq t \leq T
\end{gather}
and the gradient is
\begin{gather}
  \left\langle \J'(v),\Delta v \right\rangle_H
  = =\int_0^T \Big[
  2\big(u - \mu\big)u_x
  - \psi  a u_{xx}
  + \psi_x s'(t)
  + \psi_t
  \Big]_{x=s(t)} {\Delta s}(t)\,dt \nonumber
  \\
  +
  \left[
    \big[u - w\big]\big[u - w - 2\big] + 2 (s(T) - s_*)
  \right]\vert_{(x,t)=(s(T),T)} {\Delta s}(T)\nonumber
  \\
  + \int_0^T {\Delta a} \left[
    \int_0^{s(t)}  u_{xx} \psi \,dx
    - [u_x \psi]_{x=0}
    - [u_x\psi]_{x=s(t)}
  \right]\,dt\nonumber
\end{gather}

To create synthetic problem data to test our algorithm, we introduce a time grid
\verb+tmesh+ and evaluate the functions $a(t)$ and $s(t)$ on the given time grid
to form vectors \verb+svals+ and \verb+avals+.
With all of the other data known, the forward problem can be solved to find
an estimate for $u(x,t)$.
The traces $u(s(t),t)$ and $u(x,T)$ are then set to $\mu$ and $w$ within the
problem initialization routine, and the rest of the problem data is discarded
before starting the gradient iteration process.
In particular, the analytic solution is not subsequently used when considering
the problem with synthetic data.

As an initial approach for $s(t)$ and $a(t)$, and given the analytic solutions
$s_{\text{true}}(t)$, $a_{\text{true}}(t)$, we define
\begin{equation}
  s_{\text{linear}}(t) = s_0 + \frac{t}{T}\left(s_{\text{true}}(T) - s_0\right)
\end{equation}
where it is assumed that a measurement (or an approximation) of the
free boundary is available at the final moment.
For any fixed parameter $\lambda_s$ we set
\begin{equation}
  s_{\text{initial}}(t) = \lambda_s s_{\text{linear}}(t) + \big(1-\lambda_s\big)s_{\text{true}}(t)
\end{equation}
Since the gradient descent method is local in nature, this choice of the initial
approach allows the empirical determination of the convergence region.
Since our example has a constant ``true'' function $a(t)$, we give a quadratic
perturbation to this function in order to deduce similar information for this
control; hence, for any $\lambda_a$ we define
\begin{equation}
  a_{\text{initial}}(t) = a_{\text{true}}
  + \frac{4}{T}\lambda_a t (T-t)
\end{equation}
Note that
\[
  \max_t \vert a_{\text{initial}} - a_{\text{true}} \vert
  = \frac{4}{T} \vert \lambda_a \vert t (T-t) \vert_{t=T/2}
  = \vert \lambda_a \vert
\]
In particular, $\lambda_a$ should be bounded from below in order to ensure the
ellipticity condition $a(t) \geq a_0$ is preserved, and $\lambda_s$ cannot be
chosen arbitrarily, since we require $s(t) \geq \delta > 0$.

The process of deriving synthetic data and choosing the initial approach along
with any other parameters is encapsulated in the function \verb+initial_setup+,
included below in Section~\ref{sec:code-listing-initial-setup}.

\subsection{Tikhonov \& Samarskii Example}

A classical model problem in the form considered can be found in the text by Tikhonov \& Samarskii~\cite[Ch.\ II, App.\ IV, pp.\ 283--288]{tikhonov63}.
The problem originates in the following: find \(\big(u, \xi{}\big)\) satisfying
\begin{gather}
  \D{u_1}{t} - k_1 \D{^2 u_1}{x^2} = 0,~0<x<\xi(t),~t>0\label{eq:tikhonov-samarskii-pde-1}
  \\
  \D{u_2}{t} - k_2 \D{^2 u_2}{x^2} = 0,~\xi(t)<x<\infty,~t>0\label{eq:tikhonov-samarskii-pde-2}
  \\
  u_1(0,t) = c_1,~t>0
  \\
  u_2(x,0) = c_2,~x>\xi(0)
  \\
  u_1(x,0) = c_1,~x<\xi(0) % Note: this condition is added since $\xi$ is
  % unknown a-priori.
  \\
  u_1(\xi(t),t) = u_2(\xi(t),t)=0,~t>0
  \\
  k_1 \D{u_1}{x}(\xi(t),t) - k_2 \D{u_2}{x}(\xi(t),t) = \gamma \dD{\xi}{t}(t)\label{eq:tikhonov-samarskii-stefancond}
\end{gather}
where \(k_1, k_2, \gamma >0\) are given, and without loss of generality \(c_1<0\), \(c_2\geq 0\).
Direct calculation verifies that the exact solution to~\eqref{eq:tikhonov-samarskii-pde-1}--\eqref{eq:tikhonov-samarskii-stefancond} is
\begin{gather}
  u_1(x,t)=c_1 + B_1 \mathrm{erf}\left( \frac{x}{\sqrt{4 k_1 t}}\right)
  \\
  u_2(x,t)=A_2 + B_2 \mathrm{erf}\left( \frac{x}{\sqrt{4 k_2 t}}\right)
  \\
  \xi(t) = \alpha \sqrt{t}
  \intertext{where}
  \mathrm{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-z^2} \,dz,
  \\
  B_1 =-\frac{c_1}{\mathrm{erf}\left(\alpha/\sqrt{4 k_1}\right)},\quad
  B_2 = \frac{c_2}{1-\mathrm{erf}\left(\alpha/\sqrt{4 k_2}\right)}
  \\
  A_2 =-\mathrm{erf}\left(\alpha/\sqrt{4 k_2}\right) B_2
  \intertext{and \(\alpha{}\) is a solution of the transcendental equation}
  % %% Directly translated
  %\frac{k_1 c_1 e^{-\frac{\alpha^2}{4 k_1}}}{\sqrt{k_1} \Phi\left(\alpha/\sqrt{4 k_1}\right)}
  %+ \frac{k_2 c_2 e^{-\frac{\alpha^2}{4 k_2}}}{\sqrt{k_2}\left[1-\Phi\left(\alpha/\sqrt{4 k_2}\right) \right]}
  %= -\frac{\gamma \sqrt{\pi}}{2} \alpha
  % %% But some cancellation happens since we chose particular values for
  % a_1/a_2 from reference.
  \frac{\sqrt{k_1} c_1 e^{-\frac{\alpha^2}{4 k_1}}}{ \mathrm{erf}\left(\alpha/\sqrt{4 k_1}\right)}
  + \frac{\sqrt{k_2} c_2 e^{-\frac{\alpha^2}{4 k_2}}}{\left[1-\mathrm{erf}\left(\alpha/\sqrt{4 k_2}\right) \right]}
  = -\frac{\gamma \sqrt{\pi}}{2} \alpha
\end{gather}
The equation above has a unique solution \(\alpha>0\).

To write the problem as a one-phase Stefan problem, we simply set $c_2=0$, so $u_2 \equiv 0$, and the function $u=u_1$ satisfies
\begin{gather}
  \D{u}{t} - k_1 \D{^2 u}{x^2} = 0,~0<x<\xi(t),~t>0
  \\
  u(0,t) = c_1,~t>0
  \\
  u(x,0) = c_1,~x<\xi(0) % Note: this condition is added since $\xi$ is
  % unknown a-priori.
  \\
  u(\xi(t),t) = 0,~t>0
  \\
  k_1 \D{u}{x}(\xi(t),t) = \gamma \dD{\xi}{t}(t)
\end{gather}
where \(k_1, k_2, \gamma >0\) are given.
The analytic solution is
\begin{gather}
  u(x,t)=c_1 + B_1 \mathrm{erf}\left( \frac{x}{\sqrt{4 k_1 t}}\right)
  \\
  \xi(t) = \alpha \sqrt{t}
  \intertext{where}
  \mathrm{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-z^2} \,dz,
  \\
  B_1 =-\frac{c_1}{\mathrm{erf}\left(\alpha/\sqrt{4 k_1}\right)},\quad
  \intertext{and \(\alpha{}\) is a solution of the transcendental equation}
  \frac{\sqrt{k_1} c_1 e^{-\frac{\alpha^2}{4 k_1}}}{ \mathrm{erf}\left(\alpha/\sqrt{4 k_1}\right)}
  = -\frac{\gamma \sqrt{\pi}}{2} \alpha
\end{gather}

We calculate
\begin{gather}
  u_x(x,t) = \frac{B_1}{\sqrt{\pi k_1 t}} \exp\left(-\frac{x^2}{4 k_1 t}\right)
  \intertext{so}
  k_1 u_x(s(t),t)
  %= B_1 \frac{k_1}{\sqrt{\pi k_1 t}} \exp\left(- \frac{\alpha^2 t}{4 k_1 t}\right)
  %\\
  = B_1 \frac{k_1}{\sqrt{\pi k_1 t}} \exp\left(- \frac{\alpha^2}{4 k_1}\right)
  \\
  = \frac{c_1 \sqrt{k_1} \exp\left(-\frac{\alpha^2}{4 k_1}\right)}{\mathrm{erf}\left(\alpha/\sqrt{4 k_1}\right)}
  \left(\frac{-1}{\sqrt{\pi t}}\right)
  \intertext{By virtue of $\alpha$ satisfying the above transcendental equation and $\dD{\xi}{t} \equiv \frac{\alpha}{2 \sqrt{t}}$, it follows that}
  u_x(s(t),t)
  = \gamma \frac{\sqrt{\pi}}{2} \alpha \left(\frac{1}{\sqrt{\pi t}}\right)
  \\
  = \gamma \frac{\alpha }{2 \sqrt{t}}
  =  \gamma \dD{\xi}{t}
\end{gather}
That is, the Neumann boundary condition is satisfied at $x=s(t)$.
Similarly,
\begin{gather*}
  k_1 u_x(0,t) = \frac{ k_1 B_1 }{\sqrt{\pi k_1 t}} = \frac{\sqrt{k_1} B_1}{\sqrt{\pi t}}
\end{gather*}
is the condition on the fixed boundary.

Note that the domain degenerates at $t=0$ (and the problem data becomes
singular), which is not supported by our theoretical framework; therefore, the
data is shifted in time by a fixed amount \verb+tShift+$=: \delta$ with
$0<\delta \ll 1$.
We also give only the data necessary to solve the Neumann problem, and choose
$k_1 \equiv 1$; the solution may still depend on $c_1<0$ and the latent heat
$\gamma$, which is denoted by \verb+latentHeat+ in the code.
Tor any trial boundary curve $\tilde{\xi}$ satisfying $\tilde{\xi}(0)
= \xi(\delta)$, the problem formulated is
\begin{gather}
  \D{u}{t} - \D{^2 u}{x^2} = 0,~0<x<\tilde{\xi}(t),~t>0
  \\
  u_x(0,t) = \frac{B_1}{\sqrt{\pi (t + \delta)}},~t>0
  \\
  \D{u}{x}(\tilde{\xi}(t),t) = \gamma \tilde{\xi}'(t)
  \\
  u(x,0) = c_1 + B_1 \mathrm{erf}\left( \frac{x}{2 \sqrt{\delta}}\right),~x<\tilde{\xi}(0)
\end{gather}

The \verb+fzero+ rootfinder is used to solve the necessary transcendental
equation to find $\alpha$ (denoted by \verb+boundary_constant+ in the code.)
This solution is implemented in \verb+true_solution+, included in
Section~\ref{sec:code-listing-true-solution}.

The test code in \verb+test_true_solution+, included in Section~\ref{sec:code-listing-test-true-solution}, simply verified that the output shape and size of the parameters is correct.

%\subsection*{Acknowledgement}

\appendix
\section*{Code Listings}

\section{pdeSolver}\label{sec:code-listing-pdeSolver}
{\small
\lstinputlisting[style=MATLAB-editor]{pdeSolver.m}
}

\section{Forward}\label{sec:code-listing-forward}
{\small
\lstinputlisting[style=MATLAB-editor]{Forward.m}
}

\section{test\_Forward}\label{sec:code-listing-test-forward}
{\small
\lstinputlisting[style=MATLAB-editor]{test_Forward.m}
}

\section{Adjoint}\label{sec:code-listing-adjoint}
{\small
\lstinputlisting[style=MATLAB-editor]{Adjoint.m}
}

\section{test\_Adjoint}\label{sec:code-listing-test-adjoint}
{\small
\lstinputlisting[style=MATLAB-editor]{test_Adjoint.m}
}

\section{Functional}\label{sec:code-listing-functional}
{\small
\lstinputlisting[style=MATLAB-editor]{Functional.m}
}

\section{grad\_s}\label{sec:code-listing-grad-s}
{\small
\lstinputlisting[style=MATLAB-editor]{grad_s.m}
}

\section{grad\_a}\label{sec:code-listing-grad-a}
{\small
\lstinputlisting[style=MATLAB-editor]{grad_a.m}
}

\section{precond}\label{sec:code-listing-precond}
{\small
\lstinputlisting[style=MATLAB-editor]{precond.m}
}

\section{true\_solution}\label{sec:code-listing-true-solution}
{\small
\lstinputlisting[style=MATLAB-editor]{true_solution.m}
}

\section{test\_true\_solution}\label{sec:code-listing-test-true-solution}
{\small
\lstinputlisting[style=MATLAB-editor]{test_true_solution.m}
}

\section{initial\_setup}\label{sec:code-listing-initial-setup}
{\small
\lstinputlisting[style=MATLAB-editor]{initial_setup.m}
}

\bibliographystyle{siam}
\bibliography{refs}

\end{document}